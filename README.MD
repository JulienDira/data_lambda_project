# ğŸ§¹ Spark Delta Lake Cleaner & Data Pipeline

Ce projet met en place un pipeline complet dâ€™intÃ©gration, traitement, et nettoyage de donnÃ©es utilisant **Apache Spark**, **Delta Lake**, et **cron**. Lâ€™ensemble tourne dans un environnement DockerisÃ© avec une architecture en modules : `producer`, `consumer`, `cleaner`, et un `data_lake` structurÃ©.

---
![alt text](docker.png)

## ğŸ—‚ï¸ Arborescence du projet
```bash
.
â”œâ”€â”€ cleaner/ # Script de nettoyage/traitement pÃ©riodique
â”œâ”€â”€ consumer/ # Lecture / agrÃ©gation / monitoring (batch + streaming)
â”‚ â””â”€â”€ components/ # Modules Python internes
â”œâ”€â”€ producer/ # Simulation ou ingestion de logs
â”œâ”€â”€ data_lake/ # RÃ©pertoire Delta Lake (stockage structurÃ©)
â”‚ â”œâ”€â”€ delta/ # Logs fusionnÃ©s (delta_lake)
â”‚ â”œâ”€â”€ logs/ # Logs bruts Spark
â”‚ â”‚ â””â”€â”€ _spark_metadata/
â”‚ â”œâ”€â”€ logs_to_merge/ # Logs en attente de traitement
â”‚ â”œâ”€â”€ merged/ # Logs fusionnÃ©s (classic)
â”‚ â””â”€â”€ metrics/ # DÃ©rivÃ©s statistiques
â”‚   â””â”€â”€ batch/ # donnÃ©es batchs
â”‚     â”œâ”€â”€ agent/
â”‚     â”œâ”€â”€ daily/
â”‚     â””â”€â”€ ip/
â”‚   â””â”€â”€ streaming/ # donnÃ©es streaming
â”‚     â”œâ”€â”€ agent/
â”‚     â”œâ”€â”€ daily/
â”‚     â””â”€â”€ ip/
â”‚
â”œâ”€â”€ log/
â”‚ â”œâ”€â”€ cron/ # Logs gÃ©nÃ©rÃ©s par cron
â”‚ â””â”€â”€ writer/ # Logs du writer/producer
```

---

## ğŸš€ Objectif du projet

- Simuler la production de logs
- Stocker les donnÃ©es dans un **Data Lake Delta**
- Mettre en place un traitement **batch** et **streaming**
- Nettoyer et agrÃ©ger les donnÃ©es pÃ©riodiquement (via `cron`)
- Assurer la traÃ§abilitÃ© via des **logs persistants**

---

## âš™ï¸ Composants clÃ©s

### ğŸ­ `producer/`
- Simule ou produit des Ã©vÃ©nements (logs avec IP, timestamp, etc.)
- Ã‰crit dans `data_lake/logs_to_merge/` en format Parquet ou JSON

### ğŸ“¥ `consumer/`
- Lit les logs bruts ou agrÃ©gÃ©s
- Applique des transformations via Spark
- GÃ¨re des mÃ©triques par `agent`, `ip`, `daily` dans `/metrics/streaming/`

### ğŸ§¹ `cleaner/`
- ExÃ©cute un job Spark toutes les minutes (ou selon frÃ©quence configurÃ©e)
- Lit et nettoie les fichiers de `logs_to_merge/`, les fusionne dans `merged/`
- Enregistre des traces dans `log/cron/`

---

## ğŸ§¾ Exemple de traitement dans `cleaner/delta_lake.py`

```python
from pyspark.sql import SparkSession
from datetime import datetime

spark = SparkSession.builder \
    .appName("DeltaLakeCleaner") \
    .getOrCreate()

input_path = "/app/data_lake/logs_to_merge"
output_path = "/app/data_lake/merged"

df = spark.read.format("parquet").load(input_path)

df_clean = df.dropna().dropDuplicates()

df_clean.write.format("delta").mode("append").save(output_path)
